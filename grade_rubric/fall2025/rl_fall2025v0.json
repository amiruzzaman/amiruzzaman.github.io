{
  "Part 0": {
    "title": "Requirements",
    "problems": [
      [
        {
          "yes": "You wrote the report in LaTeX on Overleaf and the project exists.",
          "no": "You did not write the report in LaTeX on Overleaf or the project is missing.",
          "point": 2
        },
        {
          "yes": "You provided a valid GitHub commit hash from GT Enterprise GitHub.",
          "no": "You did not provide a valid GitHub commit hash (repo URL alone does not count).",
          "point": 2
        },
        {
          "yes": "You provided clear instructions in the DOCSTRING for running the code (environment, commands, initialization details).",
          "no": "Your DOCSTRING file does not include required instructions to run the code.",
          "point": 2
        },
        {
          "yes": "Your DOCSTRING reproducibility file is submitted as a PDF.",
          "no": "Your DOCSTRING reproducibility file is not a PDF.",
          "point": 1
        },
        {
          "yes": "Your report is 8 pages or fewer.",
          "no": "Your report exceeds 8 pages.",
          "point": 1
        },
        {
          "yes": "Your graphs and text are legible at 100% zoom.",
          "no": "Your graphs and/or text are not legible at 100% zoom.",
          "point": 1
        },
        {
          "yes": "Your references use a consistent format (APA, MLA, IEEE) and include at least one external source beyond course materials.",
          "no": "Your references are inconsistent, incorrectly formatted, or lack external sources.",
          "point": 1
        }
      ]
    ]
  },

  "Part 1": {
    "title": "Hypothesis",
    "problems": [
      [
        {
          "yes": "You stated at least one clear, testable hypothesis about algorithm–environment behavior.",
          "no": "You did not provide a clear, testable hypothesis.",
          "point": 2
        },
        {
          "yes": "You provided explicit evidence grounding your hypothesis (lecture content, RL theory, Sutton & Barto, or a paper).",
          "no": "You did not ground the hypothesis in prior evidence or theory.",
          "point": 2
        },
        {
          "yes": "You returned to the hypothesis in the discussion or conclusion and evaluated whether it held.",
          "no": "You did not follow up on your hypothesis in the discussion or conclusion.",
          "point": 2
        },
        {
          "yes": "You cited specific experimental evidence (plots, tables, metrics) when revisiting the hypothesis.",
          "no": "You did not cite specific evidence when discussing whether the hypothesis held.",
          "point": 2
        }
      ]
    ]
  },

  "Part 2": {
    "title": "MDP Overview and Discretization",
    "problems": [
      [
        {
          "yes": "You correctly described the Blackjack state tuple (player sum, dealer up card, usable-ace flag).",
          "no": "You did not correctly describe the Blackjack state tuple.",
          "point": 2
        },
        {
          "yes": "You correctly described the Blackjack action space: hit / stick.",
          "no": "You did not correctly describe the Blackjack action space.",
          "point": 2
        },
        {
          "yes": "You correctly described the Blackjack reward structure: +1 win, 0 draw, -1 loss.",
          "no": "You did not correctly describe the Blackjack reward structure.",
          "point": 2
        },
        {
          "yes": "You explained the episodic nature of Blackjack and how episodes terminate.",
          "no": "You did not explain the episodic nature or termination in Blackjack.",
          "point": 2
        },

        {
          "yes": "You listed the four continuous CartPole state variables.",
          "no": "You did not correctly list the CartPole state variables.",
          "point": 2
        },
        {
          "yes": "You described the CartPole action space (left/right force).",
          "no": "You did not correctly describe the CartPole action space.",
          "point": 2
        },
        {
          "yes": "You described the CartPole reward (+1 per timestep until failure).",
          "no": "You did not correctly describe the CartPole reward.",
          "point": 2
        },
        {
          "yes": "You described the CartPole termination conditions (angle/position thresholds).",
          "no": "You did not describe CartPole termination conditions.",
          "point": 2
        },

        {
          "yes": "You highlighted the discrete vs continuous nature of Blackjack vs CartPole.",
          "no": "You did not highlight discrete vs continuous differences.",
          "point": 2
        },
        {
          "yes": "You explained why discretization is required for CartPole and not Blackjack.",
          "no": "You did not explain the implications of discretization.",
          "point": 2
        },
        {
          "yes": "You described your discretization method (bins, clamping, mapping).",
          "no": "You did not describe your discretization method.",
          "point": 2
        },
        {
          "yes": "You justified your bin choices with fidelity/runtime trade-offs.",
          "no": "You did not justify your discretization choices.",
          "point": 2
        }
      ]
    ]
  },

  "Part 3": {
    "title": "Value Iteration and Policy Iteration",
    "problems": [
      [
        {
          "yes": "You described Bellman optimality updates for Value Iteration.",
          "no": "You did not correctly describe Value Iteration.",
          "point": 2
        },
        {
          "yes": "You described the policy evaluation and greedy improvement loop in Policy Iteration.",
          "no": "You did not correctly describe Policy Iteration.",
          "point": 2
        },
        {
          "yes": "You discussed convergence guarantees for VI and PI on finite MDPs.",
          "no": "You did not discuss convergence guarantees.",
          "point": 2
        },
        {
          "yes": "You contrasted VI vs PI computational tradeoffs.",
          "no": "You did not contrast VI and PI.",
          "point": 2
        },

        {
          "yes": "You provided a VI–Blackjack convergence plot (ΔV vs iteration).",
          "no": "You did not provide a VI–Blackjack convergence plot.",
          "point": 2
        },
        {
          "yes": "You provided a PI–Blackjack policy-change plot.",
          "no": "You did not provide a PI–Blackjack policy-change plot.",
          "point": 2
        },
        {
          "yes": "You provided a VI–CartPole convergence plot (ΔV with discretization noted).",
          "no": "You did not provide a VI–CartPole convergence plot.",
          "point": 2
        },
        {
          "yes": "You provided a PI–CartPole policy-change plot.",
          "no": "You did not provide a PI–CartPole policy-change plot.",
          "point": 2
        },

        {
          "yes": "You explored hyperparameter variation (θ, γ) with 3+ values.",
          "no": "You did not explore hyperparameter variation sufficiently.",
          "point": 2
        },
        {
          "yes": "You explained why PI often converges in fewer iterations.",
          "no": "You did not explain PI's faster convergence.",
          "point": 2
        },
        {
          "yes": "You reported iteration counts and explicit convergence criteria.",
          "no": "You did not report iteration counts or criteria.",
          "point": 1
        },
        {
          "yes": "You provided quantitative analysis connected to theory.",
          "no": "Your convergence analysis was not theoretically grounded.",
          "point": 2
        },

        {
          "yes": "You stated whether VI and PI produced the same policy.",
          "no": "You did not state whether VI and PI produced the same policy.",
          "point": 2
        },
        {
          "yes": "You explained differences (if any) due to tie-breaking, rounding, or tolerances.",
          "no": "You did not explain policy differences.",
          "point": 1
        },
        {
          "yes": "You referenced a visualization comparing the policies.",
          "no": "You did not include a policy comparison visualization.",
          "point": 1
        },

        {
          "yes": "You acknowledged discretization errors for CartPole.",
          "no": "You did not acknowledge discretization error.",
          "point": 1
        },
        {
          "yes": "You discussed bin trade-offs with quantitative reasoning.",
          "no": "You did not discuss discretization trade-offs meaningfully.",
          "point": 2
        }
      ]
    ]
  },

  "Part 4": {
    "title": "SARSA and Q-Learning",
    "problems": [
      [
        {
          "yes": "You correctly described the SARSA and Q-Learning update rules and their on-/off-policy nature with behavioral implications.",
          "no": "You did not correctly describe SARSA/Q-Learning or their on-/off-policy behavior.",
          "point": 4
        },

        {
          "yes": "You included four plots (SARSA/Q-Learning × Blackjack/CartPole) with performance trajectories and stability diagnostics.",
          "no": "You did not include required learning and stability plots.",
          "point": 8
        },

        {
          "yes": "You described an exploration strategy (e.g., epsilon schedule) with explicit parameters and justified its effects with evidence.",
          "no": "You did not describe or justify your exploration strategy adequately.",
          "point": 6
        },

        {
          "yes": "You compared SARSA vs Q-Learning behaviors and connected differences to theory, exploration schedule, and VI/PI.",
          "no": "You did not provide meaningful SARSA vs Q-Learning policy analysis.",
          "point": 6
        },

        {
          "yes": "You conducted hyperparameter sensitivity analysis across α, γ, and exploration parameters, with discussion of robustness.",
          "no": "Your hyperparameter sensitivity analysis was missing or insufficient.",
          "point": 4
        }
      ]
    ]
  },

  "Part 5": {
    "title": "Extra Credit: DQN or Rainbow Component",
    "problems": [
      [
        {
          "yes": "You implemented a working DQN with replay buffer and target network. Also, at least one Rainbow component or ablation, with SARSA comparisons.",
          "no": "You did not provide a substantive DQN or Rainbow-style implementation.",
          "point": 5
        }
      ]
    ]
  }
}
