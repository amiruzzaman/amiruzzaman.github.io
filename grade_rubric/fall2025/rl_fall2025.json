{
  "Part 0": {
    "title": "Requirements",
    "predefined_comments": [
      "Please check formatting consistency in your references.",
      "Ensure all required files are submitted in the correct format.",
      "DOCSTRING should include clearer environment setup instructions.",
      "Good work! Very clean DOCSTRING and provided information we needed.",
      "I am not taking points out for this, but make sure that next time you add detailed instructions to run your code. Some extra effort will help us know you are helping us to run your code to verify your experiment.",
      "For your plots and graphs, please increase the font size next time so that labels and titles are easier to read.",
      "Some of your figures are difficult to read due to their small size. I will not deduct any points for this issue this time. However, please pay attention to making your images, figures, and plots clearer in the future. You may consider increasing the size of legend text or other annotations to improve readability."
    ],
    "problems": [
      [
        {
          "yes": "You wrote the report in LaTeX on Overleaf and the project exists.",
          "no": "You did not write the report in LaTeX on Overleaf or the project is missing.",
          "point": 2
        },
        {
          "yes": "You provided a valid GitHub commit hash from GT Enterprise GitHub.",
          "no": "You did not provide a valid GitHub commit hash (repo URL alone does not count).",
          "point": 2
        },
        {
          "yes": "You provided clear instructions in the DOCSTRING for running the code (environment, commands, initialization details).",
          "no": "Your DOCSTRING file does not include required instructions to run the code.",
          "point": 2
        },
        {
          "yes": "Your DOCSTRING reproducibility file is submitted as a PDF.",
          "no": "Your DOCSTRING reproducibility file is not a PDF.",
          "point": 1
        },
        {
          "yes": "Your report is 8 pages or fewer.",
          "no": "Your report exceeds 8 pages.",
          "point": 1
        },
        {
          "yes": "Your graphs and text are legible at 100% zoom.",
          "no": "Your graphs and/or text are not legible at 100% zoom.",
          "point": 1
        },
        {
          "yes": "Your references use a consistent format (APA, MLA, IEEE) and include at least one external source beyond course materials.",
          "no": "Your references are inconsistent, incorrectly formatted, or lack external sources.",
          "point": 1
        }
      ]
    ]
  },

  "Part 1": {
    "title": "Hypothesis",
    "predefined_comments": [
      "You could strengthen your hypotheses by grounding them in course materials or relevant outside sources. This would provide a stronger theoretical basis and demonstrate how your hypotheses connect to established knowledge.",
      "You should refer to some of your results to support or refute your hypotheses. Consider incorporating this approach to improve your report in the future."
    ],
    "problems": [
      [
        {
          "yes": "You stated at least one clear, testable hypothesis about algorithm–environment behavior.",
          "no": "You did not provide a clear, testable hypothesis.",
          "point": 2
        },
        {
          "yes": "You provided explicit evidence grounding your hypothesis (lecture content, RL theory, Sutton & Barto, or a paper).",
          "no": "You did not ground the hypothesis in prior evidence or theory.",
          "point": 2
        },
        {
          "yes": "You returned to the hypothesis in the discussion or conclusion and evaluated whether it held.",
          "no": "You did not follow up on your hypothesis in the discussion or conclusion.",
          "point": 2
        },
        {
          "yes": "You cited specific experimental evidence (plots, tables, metrics) when revisiting the hypothesis.",
          "no": "You did not cite specific evidence when discussing whether the hypothesis held.",
          "point": 2
        }
      ]
    ]
  },

  "Part 2": {
    "title": "MDP Overview and Discretization",
    "predefined_comments": [
  "Good contrast between discrete Blackjack and continuous CartPole state spaces.",
  "Consider adding specific examples of how continuous CartPole values map to discrete states.",
  "Your discretization justification would benefit from quantitative analysis of bin size trade-offs.",
  "Well-explained termination conditions and reward structures for both environments.",
  "The MDP descriptions are accurate but could be more comprehensive with state transition details."
],
    "problems": [
      [
        {
          "yes": "You correctly described the Blackjack state tuple (player sum, dealer up card, usable-ace flag).",
          "no": "You did not correctly describe the Blackjack state tuple.",
          "point": 2
        },
        {
          "yes": "You correctly described the Blackjack action space: hit / stick.",
          "no": "You did not correctly describe the Blackjack action space.",
          "point": 2
        },
        {
          "yes": "You correctly described the Blackjack reward structure: +1 win, 0 draw, -1 loss.",
          "no": "You did not correctly describe the Blackjack reward structure.",
          "point": 2
        },
        {
          "yes": "You explained the episodic nature of Blackjack and how episodes terminate.",
          "no": "You did not explain the episodic nature or termination in Blackjack.",
          "point": 2
        },

        {
          "yes": "You listed the four continuous CartPole state variables.",
          "no": "You did not correctly list the CartPole state variables.",
          "point": 2
        },
        {
          "yes": "You described the CartPole action space (left/right force).",
          "no": "You did not correctly describe the CartPole action space.",
          "point": 2
        },
        {
          "yes": "You described the CartPole reward (+1 per timestep until failure).",
          "no": "You did not correctly describe the CartPole reward.",
          "point": 2
        },
        {
          "yes": "You described the CartPole termination conditions (angle/position thresholds).",
          "no": "You did not describe CartPole termination conditions.",
          "point": 2
        },

        {
          "yes": "You highlighted the discrete vs continuous nature of Blackjack vs CartPole.",
          "no": "You did not highlight discrete vs continuous differences.",
          "point": 2
        },
        {
          "yes": "You explained why discretization is required for CartPole and not Blackjack.",
          "no": "You did not explain the implications of discretization.",
          "point": 2
        },
        {
          "yes": "You described your discretization method (bins, clamping, mapping).",
          "no": "You did not describe your discretization method.",
          "point": 2
        },
        {
          "yes": "You justified your bin choices with fidelity/runtime trade-offs.",
          "no": "You did not justify your discretization choices.",
          "point": 2
        }
      ]
    ]
  },

"Part 3": {
  "title": "Value Iteration and Policy Iteration",
  "predefined_comments": [
  "Good comparison of VI vs PI convergence behavior and computational trade-offs.",
  "Consider analyzing why the algorithms produced different/same policies for each environment.",
  "Your hyperparameter analysis would benefit from clearer connections to convergence speed and policy quality.",
  "Well-documented convergence plots with appropriate metrics (ΔV, policy changes).",
  "The theoretical explanations are solid but could better connect to your empirical results."
],
  "problems": [
    [
      {
        "yes": "You described Bellman optimality updates for Value Iteration.",
        "no": "You did not correctly describe Value Iteration.",
        "point": 2
      },
      {
        "yes": "You described the policy evaluation and greedy improvement loop in Policy Iteration.",
        "no": "You did not correctly describe Policy Iteration.",
        "point": 2
      },
      {
        "yes": "You discussed convergence guarantees for VI and PI on finite MDPs.",
        "no": "You did not discuss convergence guarantees.",
        "point": 2
      },
      {
        "yes": "You contrasted VI vs PI computational tradeoffs.",
        "no": "You did not contrast VI and PI.",
        "point": 2
      },
      {
        "yes": "You provided a VI–Blackjack convergence plot (ΔV vs iteration).",
        "no": "You did not provide a VI–Blackjack convergence plot.",
        "point": 2
      },
      {
        "yes": "You provided a PI–Blackjack policy-change plot.",
        "no": "You did not provide a PI–Blackjack policy-change plot.",
        "point": 2
      },
      {
        "yes": "You provided a VI–CartPole convergence plot (ΔV with discretization noted).",
        "no": "You did not provide a VI–CartPole convergence plot.",
        "point": 2
      },
      {
        "yes": "You provided a PI–CartPole policy-change plot.",
        "no": "You did not provide a PI–CartPole policy-change plot.",
        "point": 2
      },
      {
        "yes": "You explored hyperparameter variation (θ, γ) with 3+ values.",
        "no": "You did not explore hyperparameter variation sufficiently.",
        "point": 2
      },
      {
        "yes": "You explained why PI often converges in fewer iterations.",
        "no": "You did not explain PI's faster convergence.",
        "point": 2
      },
      {
        "yes": "You reported iteration counts and explicit convergence criteria.",
        "no": "You did not report iteration counts or criteria.",
        "point": 1
      },
      {
        "yes": "You provided quantitative analysis connected to theory.",
        "no": "Your convergence analysis was not theoretically grounded.",
        "point": 2
      },
      {
        "yes": "You stated whether VI and PI produced the same policy.",
        "no": "You did not state whether VI and PI produced the same policy.",
        "point": 2
      },
      {
        "yes": "You explained differences (if any) due to tie-breaking, rounding, or tolerances.",
        "no": "You did not explain policy differences.",
        "point": 1
      },
      {
        "yes": "You referenced a visualization comparing the policies.",
        "no": "You did not include a policy comparison visualization.",
        "point": 1
      },
      {
        "yes": "You acknowledged discretization errors for CartPole.",
        "no": "You did not acknowledge discretization error.",
        "point": 1
      },
      {
        "yes": "You discussed bin trade-offs with quantitative reasoning.",
        "no": "You did not discuss discretization trade-offs meaningfully.",
        "point": 2
      }
    ]
  ]
}
,

  "Part 4": {
    "title": "SARSA and Q-Learning",
     "predefined_comments": [
  "Good explanation of on-policy vs off-policy differences between SARSA and Q-Learning.",
  "Consider analyzing how exploration strategy choices affected final policy quality in each environment.",
  "Your hyperparameter analysis would benefit from clearer connections to learning stability and convergence.",
  "Well-documented learning curves showing performance trends and algorithm comparison.",
  "The behavioral implications of on/off-policy learning could be more thoroughly discussed."
],
    "problems": [
      [
        {
          "yes": "You correctly described the SARSA and Q-Learning update rules and their on-/off-policy nature with behavioral implications.",
          "no": "You did not correctly describe SARSA/Q-Learning or their on-/off-policy behavior.",
          "point": 4
        },

        {
          "yes": "You included four plots (SARSA/Q-Learning × Blackjack/CartPole) with performance trajectories and stability diagnostics.",
          "no": "You did not include required learning and stability plots.",
          "point": 8
        },

        {
          "yes": "You described an exploration strategy (e.g., epsilon schedule) with explicit parameters and justified its effects with evidence.",
          "no": "You did not describe or justify your exploration strategy adequately.",
          "point": 6
        },

        {
          "yes": "You compared SARSA vs Q-Learning behaviors and connected differences to theory, exploration schedule, and VI/PI.",
          "no": "You did not provide meaningful SARSA vs Q-Learning policy analysis.",
          "point": 6
        },

        {
          "yes": "You conducted hyperparameter sensitivity analysis across α, γ, and exploration parameters, with discussion of robustness.",
          "no": "Your hyperparameter sensitivity analysis was missing or insufficient.",
          "point": 4
        }
      ]
    ]
  },

  "Part 5": {
    "title": "Extra Credit: DQN or Rainbow Component",
     "predefined_comments": [
  "Good implementation of DQN with proper components (replay buffer, target network).",
  "Consider analyzing how the Rainbow component improved performance compared to baseline DQN.",
  "Your comparison with SARSA would benefit from discussing sample efficiency differences.",
  "Well-documented implementation showing understanding of deep RL concepts.",
  "The ablation study could more clearly quantify the impact of each component."
],
    "problems": [
      [
        {
          "yes": "You implemented a working DQN with replay buffer and target network. Also, at least one Rainbow component or ablation, with SARSA comparisons.",
          "no": "You did not provide a substantive DQN or Rainbow-style implementation.",
          "point": 5
        }
      ]
    ]
  }
}
