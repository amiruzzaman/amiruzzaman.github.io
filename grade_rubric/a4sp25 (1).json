{
  "Part 0": {
    "title": "Requirements:",
    "problems": [
      [
        {
          "yes": "The README is present on Canvas and you provided your steps, and your implementation is accessible. .",
          "no": "There is no README file or not accessible.",
          "point": 5
        },
        {
          "yes": "You provided your Overleaf link for your report.",
          "no": "You did not provide your Overleaf link for your report.",
          "point": 1
        },
        {
          "yes": "You have provided final GitHub commit for your repo.",
          "no": "Your submission is missing final GitHub commit for your repo.",
          "point": 1
        },
        {
          "yes": "You used proper referencing style.",
          "no": "You did not follow proper referencing style.",
          "point": 1
        },
        {
          "yes": "Your graphs/figures are readable.",
          "no": "Your graphs/figures are not readable.",
          "point": 1
        },
        {
          "yes": "Also, your report is 8 pages.",
          "no": "Also, your report is ... pages.",
          "point": 1
        }
      ]
    ]
  },
  "Part 1": {
    "title": "MDP Overview and Discretization:",
    "problems": [
      [
        {
          "yes": "You defined state space, action (hit or stick), reward structure (+1 for win, 0 for tie/draw, and -1 for loss), terminal state.",
          "no": "You did not provide description of MDP problem 1.",
          "point": 5
        },
        {
          "yes": "You also defined state variables, action space, reward, and terminal conditions, discretization approach (bins, rounding methods), and justified tradeoffs between fidelity and computational cost.",
          "no": "You did not provide description of MDP problem 2.",
          "point": 5
        },
        {
          "yes": "You highlighted key differences (Blackjack: small/discrete; CartPole: large/continuous), provided implications, described discretization approach (bins, rounding methods), and justified tradeoffs between fidelity and computational cost.",
          "no": "You did not discuss why the problems are interesting and different from each other.",
          "point": 5
        }
      ]
    ]
  },
  "Part 2": {
    "title": "Value Iteration and Policy Iteration:",
    "problems": [
      [
        {
          "yes": "For VI you described Bellman optimality, for PI explained policy evaluation and improvement steps, mentioned convergence guarantees, and articulated difference between VI and PI.",
          "no": "We wanted to see some discussion of whether/how quickly VI converges.",
          "point": 8
        },
        {
          "yes": "Provided vocalization for VI on Blackjack, PI on Blackjack, VI on Cartpole, and PI on Cartpole.",
          "no": "We wanted to see some explanation for VI convergence results.",
          "point": 8
        },
        {
          "yes": "You tested hyperparameters, discussed PI's faster convergence, reported number of iterations with convergence criteria, and included quantitative and qualitative comparison.",
          "no": "You needed to add some discussion of whether/how quickly PI converges.",
          "point": 7
        },
        {
          "yes": "You stated whether VI and PI policies are the same, provided reasons for differences explained (if applicable), and used visualization.",
          "no": "We wanted to see some explanation for PI convergence results.",
          "point": 4
        },
        {
          "yes": "In addition, you discussed discretization errors and explored tradeoffs (finer binning vs. runtime/memory).",
          "no": "You needed to show a detailed comparison of final policy found by VI and PI. You could have used this question to guide your discussion, for example, \"are they the same?\"",
          "point": 3
        }
      ]
    ]
  },
  "Part 3": {
    "title": "SARSA and Q-Learning:",
    "problems": [
      [
        {
          "yes": "You provided correct definitions, clear explanation of on/off-policy, exploration implications, behavioral differences.",
          "no": "We wanted to see some discussion of whether/how quickly VI converges.",
          "point": 6
        },
        {
          "yes": "Also, you provided SARSA on Blackjack: Line plot of cumulative reward per episode, provided some discussion on Q-Learning on Blackjack: Same as SARSA, SARSA on CartPole: Line plot of episode length or reward, and Q-Learning on CartPole: Same as SARSA.",
          "no": "We wanted to see some explanation for VI convergence results.",
          "point": 8
        },
        {
          "yes": "Your report included at least one strategy used (e.g., epsilon-greedy with decay), values or decay schedules, hyperparameter differences (3+ values), and impact on convergence.",
          "no": "You needed to add some discussion of whether/how quickly PI converges.",
          "point": 7
        },
        {
          "yes": "Your report also included explicit comparison, key distinctions highlighted, reasoning rooted in theory.",
          "no": "We wanted to see some explanation for PI convergence results.",
          "point": 6
        },
        {
          "yes": "Clear comparison, identifies reasons for mismatches, visual comparisons included. ",
          "no": "You needed to show a detailed comparison of final policy found by VI and PI. You could have used this question to guide your discussion, for example, \"are they the same?\"",
          "point": 6
        }
      ]
    ]
  },
  "Part 4": {
    "title": "Reflection and Integration:",
    "problems": [
      [
        {
          "yes": "You have some discussion on specific challenges, detailed discussion, actionable improvements.",
          "no": "Some discussion of how size of the state space impacted performance would have been helpful to improve the quality of your report.",
          "point": 6
        },
        {
          "yes": "Your report also include synthesized findings, reference data, interpretation of plots, connects to theory.",
          "no": "",
          "point": 6
        }
      ]
    ]
  },
  "Part 5": {
    "title": "Extra Credit:",
    "problems": [
      [
        {
          "yes": "DDPG or SAC implemented. ",
          "no": "No implementation for the extra credit.",
          "point": 5
        }
      ]
    ]
  }
}